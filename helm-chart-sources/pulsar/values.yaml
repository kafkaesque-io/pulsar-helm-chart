# Namespace to deploy pulsar
# Use the Helm --namespace option to set the namespace

# Override for the name of the helm deployment
fullnameOverride: pulsar

# DNS name for loadbalancer
dnsName: pulsar.example.com 


# Global node selector
# If set, this will apply to all components
# Individual components can be set to a different node 
# selector
#nodeSelector:
#  zone: pulsar

## If persistence is enabled, components that has state will
## be deployed with PersistentVolumeClaims, otherwise, for test
## purposes, they will be deployed with emptDir
persistence: yes

# Global priority class
#
# If enabled, a new priority class will be created and applied to
# all Pulsar core pods. Priority indicates the importance of a Pod relative to other Pods. 
# If you are running Pulsar with other pods that depend on it (ex. microservices),
# using a higher priority will ensure it gets scheduled first
priorityClass:
  enabled: no
  value: 1000000

## If prometheus_persistence is enabled, prometheus will be deployed
## with PersistentVolumeClaims, otherwise, for test purposes, they
## will be deployed with emptyDir
prometheus_persistence: yes

prometheus_rbac: yes

# If default_storage is set, that storage class is the default for all 
# persistent volmes created by the chart.
#
# You can override the default_storage storage class in the
# volumes section of each component configuration (example: zookeeper.volumes.data.storageClass)
# 
# If you want to use an existing storage class as the default, then
# set existingStorageClassName, like this:

# To use an existing storage class:
# default_storage:
#   existingStorageClassName: <name>

# To use the default storage class of the k8s cluster, use the name "default"
# default_storage:
#   existingStorageClassName: default

# If you want the chart to create storage classes, then don't set
# existingStorageClass name and provide configuration values
# for the storage class. The settings vary based on cloud 
# provider. Below are examples for AWS, GCP, and Azure.

# For AWS
# default_storage:
#  provisioner: kubernetes.io/aws-ebs
#  type: gp2
#  fsType: ext4
#  extraParams:
#     iopsPerGB: "10"


# For GCP
# default_storage:
#   provisioner: kubernetes.io/gce-pd
#   type: pd-ssd
#   fsType: ext4
#   extraParams:
#      replication-type: none

# For Azure
# default_storage:
#   provisioner: kubernetes.io/azure-disk
#   fsType: ext4
#   type: managed-premium
#   extraParams:
#     storageaccounttype: Premium_LRS
#     kind: Managed
#     cachingmode: ReadOnly

# TLS
# When you enable TLS and are using a proxy, you need to expose the 
# TLS-enabled ports on the service. To allow TLS connections only, remove the plain-text ports.
# See the proxy and broker sections for details.

enableTls: no
tlsSecretName: pulsar-tls

# By default, TLS is enabled on the client- or admin-facing components (broker, proxy, websocket 
# proxy, standalone function worker).
# For added security, you can also enable TLS on the internal components (zookeeper, bookkeeper)
tls:
  # Enable TLS between ZooKeeper nodes (quorum TLS), between BookKeeper and ZooKeeper, and between
  # broker and ZooKeeper.
  # Note: The configured certificate must allow for both server and client use since it is used 
  #       for mTLS. This should be in certficate:
  #
  # X509v3 Extended Key Usage: 
  #               TLS Web Server Authentication, TLS Web Client Authentication
  # If using cert-manager, make sure your certficate includes:
  #   
  zookeeper:
    enabled: no
  # Enabled TLS between broker and BookKeeper
  bookkeeper:
    enabled: no

# secrets:
  ## If you're providing your own certificates, please use this to add the certificates as secrets
  ## key and certificate should start with -----BEGIN CERTIFICATE----- or 
  ## -----BEGIN RSA PRIVATE KEY----- # pragma: allowlist secret
  ##
  ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
  ##
  ## Note: The key should not be in PKCS 8 format even though that is the format used by Pulsar
  ##       The format will be converted by chart to PKCS 8. This is to maintain compatability with
  ##       cert-manager
  # key: |
  # certificate: |
  # caCertificate: |

# If you are using an external source to populate the TLS certifcate (ex cert-manager),
# enter the path and name to the CA cert. This is required so that the components
# within the cluster (proxy, broker, etc) can talk to each other
#
# If you are using self-signed certs, the CA will be contained within the tlsSecretName above,
# so use the following settings:
#
# tlsCaPath: /pulsar/certs
# tlsCaCert: ca.crt
# 
# If your certificate is signed by public CA (ex Let's Encrypt), then you can use
# the standard CA store from the container OS using the following settings:

tlsCaPath: /etc/ssl/certs
tlsCaCert: ca-certificates.crt

superUserRoles: superuser,admin,websocket,proxy
proxyRoles: proxy



# Enable token-based authentication and authorization
enableTokenAuth: no

# Token public key file name
tokenPublicKeyFile: my-public.key

# Token private key file name
tokenPrivateKeyFile: my-private.key

# Turn on anti affinity rules so that replica pods are spread for 
# high availablity.
# In development environments (ex. Minikube) with a single node, this needs to be disabled
enableAntiAffinity: yes

# Settings for anti-affinity. Host antiAffinity ensures that 
# replica pods are scheduled on different hosts. The
# number of hosts >= number of replicas. By default, this is 
# required, but you can set mode to "preferred" to make 
# this a preferred scheduling setting for deployments only (broker, proxy)
#
# Zone antiAffinity distributes replica pods across availability 
# zones. This is a "soft" requirement, so that in the event of 
# a failure of a zone, pods will run in a different zone
antiAffinity:
  host:
    enabled: yes
    mode: "required"
  zone:
    enabled: no
  

# Install Helm tests (run with helm test <release>)
# Enable basic tests
enableTests: no
# Also enabled extended tests
enableExtendedTests: no

# By default, Kubernetes will not restart pods when only their
# configmap is changed. This setting will restart pods when their
# configmap is changed using an annotation that calculates the
# checksum of the configmap
restartOnConfigMapChange:
  enabled: no

## which extra components to deploy
extra:
  # Pulsar proxy
  proxy: yes
  # Websocket proxy
  # 
  # This will enable a standalone WebSocket proxy that
  # runs as part of the proxy pod. 
  #
  # See the broker config section for enabling WebSocket
  # service within the broker.
  wsproxy: yes
  # Standalone functions workers
  #
  # See broker config section for information on enabling 
  # the function worker within the broker. If you should use one or the other.
  #
  # When enabling the standalone function worker, the proxy will be configured
  # to forward function API calls.
  #
  function: no 
  # Standalone state storage
  # Note that standalone state storage is experimental and requires a custom
  # image
  stateStorage: no
  pulsarSQL: no
  # DNS on proxy
  usedns: no
  dnsOnProxy: yes
  # Bookkeeper auto-recovery
  autoRecovery: yes
  # Bastion pod for administrative commands
  bastion: yes
  # Pulsar Beam for HTTP interface
  # Pulsar Beam depends on the proxy pod, so you must enable
  # that to use Beam. You need to expose the Pulsar Beam
  # port on the proxy. See the proxy section for details.
  pulsarBeam: no
  # burnell - various Pulsar proxy
  burnell: no
  # Pulsar manager UI for admin
  pulsarexpress: no
  # Zoonavigator for debugging Zookeeper
  zoonavigator: no
  # Tardigrade for decentralized blob storage
  # This runs the S3 gateway that connects to Tardigrade
  tardigrade: no
  #
  # DEPRECATED
  # Support for the monitoring extra is deprecated
  # and will be removed in a future release.
  #
  # Maintaining Helm support for prometheus and grafana in this chart
  # makes little sense when there are stable charts that are
  # actively maintained that do a better job. The Prometheus 
  # Operator Helm chart is recommended for monitoring since it 
  # has a rich feature set and the latest versions and features.
  #
  # The pulsar dashboard is deprecated by the Pulsar project. This 
  # chart includes Pulsar Express as an alternative.
  #
  # Monitoring stack (prometheus, grafana, and dashboard)
  monitoring: no
  # Zoonavigator
  zoonavigator: no
  # Pulsar manager UI for admin
  pulsarexpress: no
  
## Which images to use
# When upgrading a Pulsar cluster, it is recommended to upgrade the 
# components one at a time (zookeeper, bookkeeper, broker, etc). 
# This section allows for targeted upgrades of each component.
#
image:
  broker:
    # If not using tiered storage, you can use the smaller pulsar image for the broker
    repository: apachepulsar/pulsar-all
    pullPolicy: IfNotPresent
    tag: 2.5.0
  function:
    repository: apachepulsar/pulsar-all
    pullPolicy: IfNotPresent
    tag: 2.5.0
  zookeeper:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  bookkeeper:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  proxy:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  bastion:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  pulsarBeam:
    repository: kafkaesqueio/pulsar-beam
    pullPolicy: IfNotPresent
    tag: 0.23.21
  burnell:
    repository: kafkaesqueio/burnell
    pullPolicy: Always
    tag: 0.2.6
  burnellLogCollector:
    repository: kafkaesqueio/burnell-logcollector
    pullPolicy: IfNotPresent
    tag: 0.2.0
  pulsarexpress:
    repository: kafkaesqueio/pulsar-express
    tag: 0.5.1_k3
    pullPolicy: IfNotPresent
  pulsarSQL:
    repository: apachepulsar/pulsar-all
    tag: 2.4.2
    pullPolicy: IfNotPresent
  tardigrade:
    repository: storjlabs/gateway
    pullPolicy: IfNotPresent
    tag: latest
  # Standalone state storage is experimental and requires the custom image below 
  stateStorage:
    repository: kafkaesqueio/pulsar-all
    pullPolicy: IfNotPresent
    tag: 2.4.2-k2.1
  dashboard:
    tag: 2.5.0
  grafana:
    tag: 2.5.0
  prometheus:
    tag: v2.8.1


## Tiered Storage
##

storageOffload: {}
  ## General
  ## =======
  # bucket: <bucket>
  # region: <region>
  # maxBlockSizeInBytes: "64000000" 
  # readBufferSizeInBytes: "1000000"
  ## The following are default values for the cluster. They can be changed
  ## on each namespace.
  # managedLedgerOffloadDeletionLagMs: "14400000"
  # managedLedgerOffloadAutoTriggerSizeThresholdBytes: "-1" # disabled

  # For AWS S3
  # ======
  # You must create an IAM account with access to the bucket and 
  # generate keys for that account.
  #
  # driver: aws-s3
  # accessKey: <access-key>
  # accessSecret: <secret-key> # pragma: allowlist secret

  # For S3 Compatible
  # =================
  # Need to create access and secret key for S3 compatible service
  #
  # driver: s3
  # accessKey: <access-key>
  # accessSecret: <secret-key> # pragma: allowlist secret 
  # serviceEndpoint: host:port

  # For Tardigrade
  # =================
  # Need to enable extra.tardigrade for the S3 gateway. 
  # See tardigrade section below to configure the S3 gateway
  #
  # driver: s3
 
  

  ## For Google Cloud Storage
  ## ====================
  ## You must create a service account that has access to the objects in GCP buckets
  ## and upload its key as a JSON file to a secret.
  ## 
  ## 1. Go to https://console.cloud.google.com/iam-admin/serviceaccounts
  ## 2. Select your project.
  ## 3. Create a new service account.
  ## 4. Give the service account permission to access the bucket. For example, 
  ##    the "Storage Object Admin" role.
  ## 5. Create a key for the service account and save it as a JSON file.
  ## 6. Save the JSON file in a secret:
  ##       kubectl create secret generic pulsar-gcp-sa-secret \
  ##          --from-file=kafkaesque-223201-f12856532197.json \
  ##          --namespace pulsar
  ## 
  # driver: google-cloud-storage
  # gcsServiceAccountSecret: pulsar-gcp-sa-secret # pragma: allowlist secret
  # gcsServiceAccountJsonFile: kafkaesque-223201-f12856532197.json
 

## Pulsar: Zookeeper cluster
## templates/zookeeper-statefulset.yaml
##
zookeeper:
  component: zookeeper
  replicaCount: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  # nodeAffinity:
  #   preferredDuringSchedulingIgnoredDuringExecution:
  #   - weight: 1
  #     preference:
  #       matchExpressions:
  #       - key: failure-domain.beta.kubernetes.io/region
  #         operator: In
  #         values:
  #         - region1
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolerations: []
  gracePeriod: 60
  probe:
    enabled: yes
    initial: 10
    period: 30
  resources:
    requests:
      memory: 1Gi
      cpu: 0.3
  volumes:
    data:
      name: data
      size: 5Gi
      # You can override the default_storage class for this volume. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters under storageClass. The 
      # appropriate parameters will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none
  ## Zookeeper configmap
  ## templates/zookeeper-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms1g -Xmx1g -Dcom.sun.management.jmxremote -Djute.maxbuffer=10485760 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:+DisableExplicitGC -XX:+PerfDisableSharedMem -Dzookeeper.forceSync=no\""
    PULSAR_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  ## Zookeeper service
  ## templates/zookeeper-service.yaml
  ##
  service:
    annotations:
    ports:
    - name: server
      port: 2888
    - name: leader-election
      port: 3888
    - name: client
      port: 2181
  ## Zookeeper PodDisruptionBudget
  ## templates/zookeeper-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

## Pulsar Zookeeper metadata. The metadata will be deployed as
## soon as the las zookeeper node is reachable. The deployment
## of other components that depends on zookeeper, such as the
## bookkeeper nodes, broker nodes, etc will only start to be
## deployed when the zookeeper cluster is ready and with the
## metadata deployed
zookeeperMetadata:
  component: zookeeper-metadata

## Pulsar: Bookkeeper cluster
## templates/bookkeeper-statefulset.yaml
##
bookkeeper:
  component: bookkeeper
  replicaCount: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolerations: []
  probe:
    enabled: yes
    port: 3181
    initial: 10
    period: 30
  gracePeriod: 60
  resources:
    requests:
      memory: 2Gi
      cpu: 1
  volumes:
    journal:
      name: journal
      size: 50Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none
    ledgers:
      name: ledgers
      size: 50Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none

    # If you enable state storage on BookKeeper, a persistent volume
    # to hold the state files (ranges)
    ranges:
      name: ranges
      size: 5Gi
    # To override the default_storage class for this volume, set storageClass. 
    # To use an existing storage class, set existingStorageClassName.
    # To have the chart create a storage class, leave existingStorageClassName 
    # unset and specify the storage class parameters. The appropriate parameters
    # will vary by cloud provider. See default_storage above for examples.
    #
    # existingStorageClassName: <name>
    #
    # To use the default storage class of the k8s cluster, use the name "default"
    # existingStorageClassName: default
    #
    # OR (GCP example)
    #
    # storageClass:
    #   provisioner: kubernetes.io/gce-pd
    #   type: pd-ssd
    #   fsType: ext4
    #   extraParams:
    #      replication-type: none

    
  ## Bookkeeper configmap
  ## templates/bookkeeper-configmap.yaml
  ##
  configData:
    BOOKIE_MEM: "\"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=2g -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -verbosegc -XX:G1LogLevel=finest\""
    BOOKIE_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
    statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  ## Bookkeeper configmap
  ## templates/bookkeeper-service.yaml
  ##
  service:
    annotations:
    ports:
    - name: server
      port: 3181
  ## Bookkeeper PodDisruptionBudget
  ## templates/bookkeeper-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

## Pulsar: Broker cluster
## templates/broker-deployment.yaml
##
broker:
  component: broker
  replicaCount: 3
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
    #
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  tolerations: []
  gracePeriod: 60
  # Enable extra services in the broker
  # These can also be enabled as standalone services
  # See extras section above
  functionsWorkerEnabled: no
  webSocketServiceEnabled: no
  probe:
    enabled: yes
    port: 8080
    initial: 10
    period: 30
  resources:
    requests:
      memory: 2Gi
      cpu: 1
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=2g -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem\""
    PULSAR_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
    managedLedgerDefaultEnsembleSize: "2"
    managedLedgerDefaultWriteQuorum: "2"
    managedLedgerDefaultAckQuorum: "2"
    deduplicationEnabled: "true"
    exposeTopicLevelMetricsInPrometheus: "true"
    exposeConsumerLevelMetricsInPrometheus: "true"
    backlogQuotaDefaultRetentionPolicy: "producer_exception"
  
  ## Broker service
  ## templates/broker-service.yaml
  ##
  # If you are enabling TLS, make sure these ports are on the port list:
  #   - name: https
  #     port: 8443
  #     protocol: TCP
  #   - name: pulsarssl
  #     port: 6651
  # To only allow TLS connections, remove the plain-text ports (http, pulsar)
  service:
    annotations: {}
    type: ClusterIP
    headless: no
    ports:
    - name: http
      port: 8080
    - name: pulsar
      port: 6650
    - name: https
      port: 8443
    - name: pulsarssl
      port: 6651
  ingress:
    enabled: no
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

function:
  component: function
  replicaCount: 2
  functionReplicaCount: 2
  usePython3: no
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  probe:
    enabled: yes
    port: 6750
    initial: 10
    period: 30
  # Only the process runtime has been tested with this chart
  runtime: "process"
  # If state storage is enabled state storage will be enabled on Bookkeeper and the 
  # function workers will connect to BookKeeper by default.
  # If you enable standalone state storage (extra.stateStorage), the function workers will 
  # connect to that
  # Note: State storage is in developer preview
  # Note: Standalone state storage is experimental
  enableStateStorage: no 
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "false"
    prometheus.io/port: "8080"
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 4Gi
      cpu: 1
  volumes:
    data:
      name: logs
      size: 5Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none


  ## Function configmap
  ## templates/function-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=2g -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem\""
    PULSAR_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  ## Function service
  ## templates/function-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    headless: yes
    ports:
    - name: http
      port: 6750
    - name: https
      port: 6751
  ## Function PodDisruptionBudget
  ## templates/function-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

# Configuration for presto SQL
pulsarSQL:
  component: sql
  # nodeSelector:
  # affinity:
  # tolerations:
  server:
    scheduleCoordinator: yes
    workers: 0
    node:
      environment: production
    log:
      presto:
        level: INFO
    config:
      http:
        port: 8080
      query:
        maxMemory: "4GB"
        maxMemoryPerNode: "1GB"
    jvm:
      maxHeapSize: "8G"
      gcMethod:
        type: "UseG1GC"
        g1:
          heapRegionSize: "32M"

  image:
    securityContext:
      enabled: no
      runAsUser: 1000
      runAsGroup: 1000

  service:
    type: ClusterIP

  ingress:
    enabled: no
    host: presto.host.com
    annotations: {}

  catalog:
    pulsar:
      maxEntryReadBatchSize: "100"
      targetNumSplits: "2"
      maxSplitMessageQueueSize: "10000"
      maxSplitEntryQueueSize: "1000"
      namespaceDelimiterRewriteEnable: "false"
      rewriteNamespaceDelimiter: "/"
      bookkeeperThrottleValue: "0"
      managedLedgerCacheSizeMB: "0"

# Tardigrade S3 gateway
#
# Configure the S3 gateway according to instructions here: 
#     https://documentation.tardigrade.io/api-reference/s3-gateway
# Make sure you configure an access grant. Once you have done that, you can find the values
# for acess, accesKey, and secretKey in: 
#     ~/.local/share/storj/gatewayconfig.yaml
#
tardigrade:
  access: access-key-generated-with-uplink
  accessKey: 2J7EJY4xTK6uHKqnCE4nAhdGfXqy
  secretKey: 4YeYwYdsoFFpvtNFuncWcTVqSTPL
  service:
    port: 7777
    type: ClusterIP


stateStorage:
  component: statestorage
  replicaCount: 1
  numStorageContainers: 16
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  probe:
    enabled: yes
    port: 4181
    initial: 10
    period: 30
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "false"
    prometheus.io/port: "8080"
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 1Gi
      cpu: 0.5 
  volumes:
    data:
      name: data
      size: 5Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none
  ## StateStorage configmap
  ## templates/state-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\" -Xms1g -Xmx1g -XX:MaxDirectMemorySize=1g\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  ## Function service
  ## templates/state-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    headless: yes
    ports:
    - name: bkstate
      port: 4181


tokenServer:
  allowedRoles: superuser,token-creation

## Pulsar Extra: Proxy
## templates/proxy-deployment.yaml
##
proxy:
  component: proxy
  replicaCount: 3
  disableZookeeperDiscovery: yes
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  tolerations: []
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  gracePeriod: 60
  probe:
    enabled: yes
    initial: 10
    period: 30
  # Resources for the main Pulsar proxy
  resources:
    requests:
      memory: 1Gi
      cpu: 1
  # Resources for the websocket proxy
  wsResources:
    requests:
      memory: 1Gi
      cpu: 1
  ## Proxy configmap
  ## templates/proxy-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms1g -Xmx1g -XX:MaxDirectMemorySize=1g\""
    numHttpServerThreads: "10"
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
    # To disable authentication on the Prometheus /metrics endpoint
    # authenticateMetricsEndpoint: "false"
  wsProxyPort: 8000
  wsProxyPortTls: 8001
  ## Proxy loadbalancer service
  ## templates/proxy-service.yaml
  ##
  # service:
  #   annotations: {}
  #   type: LoadBalancer
  #   ports:
  #   - name: http
  #     port: 8080
  #     protocol: TCP
  #   - name: pulsar
  #     port: 6650
  #     protocol: TCP
  #   - name: ws
  #     port: 8000
  #     protocol: TCP
  #
  # If you are enabling TLS, add these ports:
  #   - name: https
  #     port: 8443
  #     protocol: TCP
  #   - name: pulsarssl
  #     port: 6651
  #     protocol: TCP
  #   - name: wss
  #     port: 8001
  #     protocol: TCP
  #
  # To only use TLS, remove the plain text ports from the list (http, pulsar, ws)
  #
  # If you are enabling Pulsar Beam, add this port:
  #   - name: pulsarbeam
  #     port: 8085
  #     protocol: TCP
  #
  # Pulsar Beam uses the same port for whether TLS is enabled or not

  ## Proxy cluster service
  ## templates/proxy-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    ports:
    - name: http
      port: 8080
      protocol: TCP
    - name: pulsar
      port: 6650
      protocol: TCP
    - name: ws
      port: 8000
      protocol: TCP

  ingress:
    enabled: no
    host: admin.host.com
    enableWebSocket: no
    wssPortOnProxy: 8001
    enableBurnell: no

  ## Proxy PodDisruptionBudget
  ## templates/proxy-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

## Pulsar Extra:  DNS on proxy RBAC
## templates/dns-rbac.yaml
## templates/dns-deployment.yaml
dns:
  component: dns
  provider: aws
  domainFilter: example.com 
  hostAnnotations:
  # Trying to create CNAME records (ex AWS NLB)
  # Need to add TXT prefix to prevent conflicts
  # txtPrefix: k8s-
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar

## Pulsar Extra: Bookkeeper auto-recovery
## templates/autorecovery-deployment.yaml
##
autoRecovery:
  component: autorecovery
  replicaCount: 1
  #nodeSelector:
  #  cloud.google.com/gke-nodepool: default-pool
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 512Mi
      cpu: 0.3
  ## Bookkeeper auto-recovery configmap
  ## templates/autorecovery-configmap.yaml
  ##
  configData:
    BOOKIE_MEM: "\" -Xms512m -Xmx512m \""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
## Pulsar Extra: Dashboard
## templates/dashboard-deployment.yaml
##
dashboard:
  component: dashboard
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 1024Mi
      cpu: 1
  image:
    repository: apachepulsar/pulsar-dashboard
    pullPolicy: IfNotPresent
  ## Dashboard service
  ## templates/dashboard-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 80

## Components Stack: pulsarexpress
## templates/pulsarexpress.yaml
##
pulsarexpress:
  component: pulsarexpress
  replicaCount: 1
  tlsCaPath: /etc/ssl
  tlsCaCert: cert.pem
  ingress:
    enabled: yes
    host: ui.host.com
    annotations: {}
  # nodeSelector:
  # cloud.google.com/gke-nodepool: default-pool
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  configData: {}
  ## Pulsar Express service
  ## templates/pulsarexpress-service.yaml
  ##
  service:
    type: ClusterIP
    annotations: {}
    ports:
      - name: server
        port: 3000

## Pulsar Extra: Zoonavigator
## templates/zoonavigator-deployment.yaml
##
zoonavigator:
  component: zoonavigator
  replicaCount: 1
  autoConnect: true
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 128Mi
      cpu: 0.1
  image:
    repository: 
      web: elkozmon/zoonavigator-web
      api: elkozmon/zoonavigator-api
    tag: 0.6.0
    pullPolicy: IfNotPresent

  ## Zoonavigator service
  ## templates/zoonavigator-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 8001

  ## Dashboard service
  ## templates/dashboard-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 8000
    - name: jolokia
      port: 8778


## Pulsar Extra: Bastion
## templates/bastion-deployment.yaml
##
bastion:
  component: bastion
  installDebugTools: no
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 256Mi
      cpu: 0.25
  ## Bastion configmap
  ## templates/bastion-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms256m -Xmx256m -XX:MaxDirectMemorySize=256m\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"

## Pulsar Beam
## templates/beamwh-deployment.yaml
##
pulsarBeam:
  tlsCaPath: /etc/ssl
  tlsCaCert: cert.pem
  component: pulsarbeam
  replicaCount: 1
  resources:
    requests:
      memory: 256Mi
      cpu: 0.5
  annotations: {}
  tolerations: []
  gracePeriod: 60
  # logLevel: debug
  # nodeAffinity: 
  #   requiredDuringSchedulingIgnoredDuringExecution:
  #     nodeSelectorTerms:
  #     - matchExpressions:
  #       - key: nodeppool
  #         operator: In
  #         values:
  #         - pulsar
  
## Monitoring Stack: Prometheus
## templates/prometheus-deployment.yaml
##
prometheus:
  component: prometheus
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 512Mi
      cpu: 0.5
  image:
    repository: prom/prometheus
    pullPolicy: IfNotPresent
  volumes:
    data:
      name: data
      size: 5Gi
      # storageClass:
      #   type: gp2
      #   provisioner: kubernetes.io/aws-ebs
      #   fsType: ext4
  ## Prometheus service
  ## templates/prometheus-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 9090

## Monitoring Stack: Grafana
## templates/grafana-deployment.yaml
##
grafana:
  component: grafana
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 256Mi
      cpu: 0.25
  image:
    repository: apachepulsar/pulsar-grafana
    pullPolicy: IfNotPresent
  ## Grafana service
  ## templates/grafana-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 3000

