# Namespace to deploy pulsar
# Use the Helm --namespace option to set the namespace

# Override for the name of the helm deployment
fullnameOverride: pulsar

# DNS name for loadbalancer
dnsName: pulsar.example.com 


# Global node selector
# If set, this will apply to all components
# Individual components can be set to a different node 
# selector
#nodeSelector:
#  zone: pulsar

## If persistence is enabled, components that has state will
## be deployed with PersistentVolumeClaims, otherwise, for test
## purposes, they will be deployed with emptDir
persistence: yes

## If prometheus_persistence is enabled, prometheus will be deployed
## with PersistentVolumeClaims, otherwise, for test purposes, they
## will be deployed with emptyDir
prometheus_persistence: yes

prometheus_rbac: yes

# If default_storage is set, that storage class is the default for all 
# persistent volmes created by the chart.
#
# You can override the default_storage storage class in the
# volumes section of each component configuration (example: zookeeper.volumes.data.storageClass)
# 
# If you want to use an existing storage class as the default, then
# set existingStorageClassName, like this:

# To use an existing storage class:
# default_storage:
#   existingStorageClassName: <name>

# To use the default storage class of the k8s cluster, use the name "default"
# default_storage:
#   existingStorageClassName: default

# If you want the chart to create storage classes, then don't set
# existingStorageClass name and provide configuration values
# for the storage class. The settings vary based on cloud 
# provider. Below are examples for AWS, GCP, and Azure.

# For AWS
# default_storage:
#  provisioner: kubernetes.io/aws-ebs
#  type: gp2
#  fsType: ext4
#  extraParams:
#     iopsPerGB: "10"


# For GCP
# default_storage:
#   provisioner: kubernetes.io/gce-pd
#   type: pd-ssd
#   fsType: ext4
#   extraParams:
#      replication-type: none

# For Azure
# default_storage:
#   provisioner: kubernetes.io/azure-disk
#   fsType: ext4
#   type: managed-premium
#   extraParams:
#     storageaccounttype: Premium_LRS
#     kind: Managed
#     cachingmode: ReadOnly

# TLS
# When you enable TLS and are using a proxy, you need to expose the 
# TLS-enabled ports. To allow TLS connections only, remove the plain-text ports.
# See the proxy section for details.

enableTls: no
tlsSecretName: pulsar-tls

# secrets:
  ## If you're providing your own certificates, please use this to add the certificates as secrets
  ## key and certificate should start with -----BEGIN CERTIFICATE----- or 
  ## -----BEGIN RSA PRIVATE KEY----- # pragma: allowlist secret
  ##
  ## If you're using cert-manager, this is unneeded, as it will create the secret for you if it is not set
  ##
  ## Note: The key should not be in PKCS 8 format even though that is the format used by Pulsar
  ##       The format will be converted by chart to PKCS 8. This is to maintain compatability with
  ##       cert-manager
  # key: |
  # certificate: |
  # caCertificate: |

# If you are using an external source to populate the TLS certifcate (ex cert-manager),
# enter the path and name to the CA cert. This is required so that the components
# within the cluster (proxy, broker, etc) can talk to each other
#
# If you are using self-signed certs, the CA will be contained within the tlsSecretName above,
# so use the following settings:
#
# tlsCaPath: /pulsar/certs
# tlsCaCert: ca.crt
# 
# If your certificate is signed by public CA (ex Let's Encrypt), then you can use
# the standard CA store from the container OS using the following settings:

tlsCaPath: /etc/ssl/certs
tlsCaCert: ca-certificates.crt

superUserRoles: superuser,admin,websocket,proxy
proxyRoles: proxy



# Enable token-based authentication and authorization
enableTokenAuth: no

# Token public key file name
tokenPublicKeyFile: my-public.key

# Token private key file name
tokenPrivateKeyFile: my-private.key

# Turn on anti affinity rules so that replica pods are spread for 
# high availablity.
# In development environments (ex. Minikube) with a single node, this needs to be disabled
enableAntiAffinity: yes

# Settings for anti-affinity. Host antiAffinity ensures that 
# replica pods are scheduled on different hosts. The
# number of hosts >= number of replicas
#
# Zone antiAffinity distributes replica pods across availability 
# zones. This is a "soft" requirement, so that in the event of 
# a failure of a zone, pods will run in a different zone
antiAffinity:
  host:
    enabled: yes
  zone:
    enabled: no
  

# Install Helm tests (run with helm test <release>)
enableTests: no

## which extra components to deploy
extra:
  # Pulsar proxy
  proxy: yes
  # Websocket proxy
  # 
  # This will enable a standalone WebSocket proxy that
  # runs as part of the proxy pod. 
  #
  # See the broker config section for enabling WebSocket
  # service within the broker.
  wsproxy: yes
  # Standalone functions workers
  #
  # See broker config section for information on enabling 
  # the function worker within the broker. If you should use one or the other.
  #
  # When enabling the standalone function worker, the proxy will be configured
  # to forward function API calls.
  #
  function: no 
  # Standalone state storage
  # Note that standalone state storage is experimental and requires a custom
  # image
  stateStorage: no
  # DNS on proxy
  usedns: no
  dnsOnProxy: yes
  # Bookkeeper auto-recovery
  autoRecovery: yes
  # Bastion pod for administrative commands
  bastion: yes
  # Pulsar Beam webhook brokers
  beamwh: no
  # Pulsar Beam http component and rest api
  # For external access via the proxy you need to expose
  # the Pulsar Beam port. See the proxy section for details.
  pulsarBeam: no
  # burnell - various Pulsar proxy
  burnell: no
  # Pulsar manager UI for admin
  pulsarexpress: no
  # Zoonavigator for debugging Zookeeper
  zoonavigator: no
  #
  # DEPRECATED
  # Support for the monitoring extra is deprecated
  # and will be removed in a future release.
  #
  # Maintaining Helm support for prometheus and grafana in this chart
  # makes little sense when there are stable charts that are
  # actively maintained that do a better job. The Prometheus 
  # Operator Helm chart is recommended for monitoring since it 
  # has a rich feature set and the latest versions and features.
  #
  # The pulsar dashboard is deprecated by the Pulsar project. This 
  # chart includes Pulsar Express as an alternative.
  #
  # Monitoring stack (prometheus, grafana, and dashboard)
  monitoring: no
  # Zoonavigator
  zoonavigator: no
  # Pulsar manager UI for admin
  pulsarexpress: no
  
## Which images to use
# When upgrading a Pulsar cluster, it is recommended to upgrade the 
# components one at a time (zookeeper, bookkeeper, broker, etc). 
# This section allows for targeted upgrades of each component.
#
image:
  broker:
    # If not using tiered storage, you can use the smaller pulsar image for the broker
    repository: apachepulsar/pulsar-all
    pullPolicy: IfNotPresent
    tag: 2.5.0
  function:
    repository: apachepulsar/pulsar-all
    pullPolicy: IfNotPresent
    tag: 2.5.0
  zookeeper:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  bookkeeper:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  proxy:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  bastion:
    repository: apachepulsar/pulsar
    pullPolicy: IfNotPresent
    tag: 2.5.0
  beamwh:
    repository: kafkaesqueio/pulsar-beam
    pullPolicy: IfNotPresent
    tag: 0.23.12
  pulsarBeam:
    repository: kafkaesqueio/pulsar-beam
    pullPolicy: IfNotPresent
    tag: 0.23.12
  burnell:
    repository: kafkaesqueio/burnell
    pullPolicy: IfNotPresent
    tag: 0.1.4
  pulsarexpress:
    repository: kafkaesqueio/pulsar-express
    tag: 0.5.1_k2
    pullPolicy: IfNotPresent
  # Standalone state storage is experimental and requires the custom image below 
  stateStorage:
    repository: kafkaesqueio/pulsar-all
    pullPolicy: IfNotPresent
    tag: 2.4.2-k2.1
  dashboard:
    tag: 2.5.0
  grafana:
    tag: 2.5.0
  prometheus:
    tag: v2.8.1


## Tiered Storage
##

storageOffload: {}
  ## General
  ## =======
  # bucket: <bucket>
  # region: <region>
  # maxBlockSizeInBytes: "64000000" 
  # readBufferSizeInBytes: "1000000"
  ## The following are default values for the cluster. They can be changed
  ## on each namespace.
  # managedLedgerOffloadDeletionLagMs: "14400000"
  # managedLedgerOffloadAutoTriggerSizeThresholdBytes: "-1" # disabled

  # For AWS S3
  # ======
  # You must create an IAM account with access to the bucket and 
  # generate keys for that account.
  #
  # driver: aws-s3
  # accessKey: <access-key>
  # accessSecret: <secret-key> # pragma: allowlist secret

  # For S3 Compatible
  # =================
  # Need to create access and secret key for S3 compatible service
  #
  # driver: s3
  # accessKey: <access-key>
  # accessSecret: <secret-key> # pragma: allowlist secret 
  # serviceEndpoint: host:port

  ## For Google Cloud Storage
  ## ====================
  ## You must create a service account that has access to the objects in GCP buckets
  ## and upload its key as a JSON file to a secret.
  ## 
  ## 1. Go to https://console.cloud.google.com/iam-admin/serviceaccounts
  ## 2. Select your project.
  ## 3. Create a new service account.
  ## 4. Give the service account permission to access the bucket. For example, 
  ##    the "Storage Object Admin" role.
  ## 5. Create a key for the service account and save it as a JSON file.
  ## 6. Save the JSON file in a secret:
  ##       kubectl create secret generic pulsar-gcp-sa-secret \
  ##          --from-file=kafkaesque-223201-f12856532197.json \
  ##          --namespace pulsar
  ## 
  # driver: google-cloud-storage
  # gcsServiceAccountSecret: pulsar-gcp-sa-secret # pragma: allowlist secret
  # gcsServiceAccountJsonFile: kafkaesque-223201-f12856532197.json

## Pulsar: Zookeeper cluster
## templates/zookeeper-statefulset.yaml
##
zookeeper:
  component: zookeeper
  replicaCount: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolerations: []
  gracePeriod: 60
  probe:
    enabled: yes
    initial: 10
    period: 30
  resources:
    requests:
      memory: 1Gi
      cpu: 0.3
  volumes:
    data:
      name: data
      size: 5Gi
      # You can override the default_storage class for this volume. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters under storageClass. The 
      # appropriate parameters will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none
  ## Zookeeper configmap
  ## templates/zookeeper-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms1g -Xmx1g -Dcom.sun.management.jmxremote -Djute.maxbuffer=10485760 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:+DisableExplicitGC -XX:+PerfDisableSharedMem -Dzookeeper.forceSync=no\""
    PULSAR_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  ## Zookeeper service
  ## templates/zookeeper-service.yaml
  ##
  service:
    annotations:
      service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
    ports:
    - name: server
      port: 2888
    - name: leader-election
      port: 3888
    - name: stats
      port: 2181
  ## Zookeeper PodDisruptionBudget
  ## templates/zookeeper-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

## Pulsar Zookeeper metadata. The metadata will be deployed as
## soon as the las zookeeper node is reachable. The deployment
## of other components that depends o zookeeper, such as the
## bookkeeper nodes, broker nodes, etc will only start to be
## deployed when the zookeeper cluster is ready and with the
## metadata deployed
zookeeperMetadata:
  component: zookeeper-metadata

## Pulsar: Bookkeeper cluster
## templates/bookkeeper-statefulset.yaml
##
bookkeeper:
  component: bookkeeper
  replicaCount: 3
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8000"
  tolerations: []
  probe:
    enabled: yes
    port: 3181
    initial: 10
    period: 30
  gracePeriod: 60
  resources:
    requests:
      memory: 2Gi
      cpu: 1
  volumes:
    journal:
      name: journal
      size: 50Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none
    ledgers:
      name: ledgers
      size: 50Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none

    # If you enable state storage on BookKeeper, a persistent volume
    # to hold the state files (ranges)
    ranges:
      name: ranges
      size: 5Gi
    # To override the default_storage class for this volume, set storageClass. 
    # To use an existing storage class, set existingStorageClassName.
    # To have the chart create a storage class, leave existingStorageClassName 
    # unset and specify the storage class parameters. The appropriate parameters
    # will vary by cloud provider. See default_storage above for examples.
    #
    # existingStorageClassName: <name>
    #
    # To use the default storage class of the k8s cluster, use the name "default"
    # existingStorageClassName: default
    #
    # OR (GCP example)
    #
    # storageClass:
    #   provisioner: kubernetes.io/gce-pd
    #   type: pd-ssd
    #   fsType: ext4
    #   extraParams:
    #      replication-type: none

    
  ## Bookkeeper configmap
  ## templates/bookkeeper-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=2g -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+UseG1GC -XX:MaxGCPauseMillis=10 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintGCApplicationStoppedTime -XX:+PrintHeapAtGC -verbosegc -XX:G1LogLevel=finest\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
    # dbStorage_writeCacheMaxSizeMb: "2048"
    # dbStorage_readAheadCacheMaxSizeMb: "2048"
    # journalMaxSizeMB: "2048"
    statsProviderClass: org.apache.bookkeeper.stats.prometheus.PrometheusMetricsProvider
  ## Bookkeeper configmap
  ## templates/bookkeeper-service.yaml
  ##
  service:
    annotations:
      publishNotReadyAddresses: "true"
    ports:
    - name: server
      port: 3181
  ## Bookkeeper PodDisruptionBudget
  ## templates/bookkeeper-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

## Pulsar: Broker cluster
## templates/broker-deployment.yaml
##
broker:
  component: broker
  replicaCount: 3
  updateStrategy: {}
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
    #
  #preferredZone: "us-east1-c"
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  tolerations: []
  gracePeriod: 60
  # Enable extra services in the broker
  # These can also be enabled as standalone services
  # See extras section above
  functionsWorkerEnabled: no
  webSocketServiceEnabled: no
  probe:
    enabled: yes
    port: 8080
    initial: 10
    period: 30
  resources:
    requests:
      memory: 2Gi
      cpu: 1
  ## Broker configmap
  ## templates/broker-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=2g -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem\""
    PULSAR_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
    managedLedgerDefaultEnsembleSize: "2"
    managedLedgerDefaultWriteQuorum: "2"
    managedLedgerDefaultAckQuorum: "2"
    deduplicationEnabled: "true"
    exposeTopicLevelMetricsInPrometheus: "true"
    backlogQuotaDefaultRetentionPolicy: "producer_exception"
  
  ## Broker service
  ## templates/broker-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    headless: no
    ports:
    - name: http
      port: 8080
    - name: pulsar
      port: 6650
    - name: https
      port: 8443
    - name: pulsarssl
      port: 6651
  ## Broker PodDisruptionBudget
  ## templates/broker-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

function:
  component: function
  replicaCount: 2
  functionReplicaCount: 2
  usePython3: no
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  probe:
    enabled: yes
    port: 6750
    initial: 10
    period: 30
  # Only the process runtime has been tested with this chart
  runtime: "process"
  # If state storage is enabled state storage will be enabled on Bookkeeper and the 
  # function workers will connect to BookKeeper by default.
  # If you enable standalone state storage (extra.stateStorage), the function workers will 
  # connect to that
  # Note: State storage is in developer preview
  # Note: Standalone state storage is experimental
  enableStateStorage: no 
  #preferredZone: "us-east1-c"
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "false"
    prometheus.io/port: "8080"
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 4Gi
      cpu: 1
  volumes:
    data:
      name: logs
      size: 5Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none


  ## Function configmap
  ## templates/function-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms2g -Xmx2g -XX:MaxDirectMemorySize=2g -Dio.netty.leakDetectionLevel=disabled -Dio.netty.recycler.linkCapacity=1024 -XX:+ParallelRefProcEnabled -XX:+UnlockExperimentalVMOptions -XX:+AggressiveOpts -XX:+DoEscapeAnalysis -XX:ParallelGCThreads=32 -XX:ConcGCThreads=32 -XX:G1NewSizePercent=50 -XX:+DisableExplicitGC -XX:-ResizePLAB -XX:+ExitOnOutOfMemoryError -XX:+PerfDisableSharedMem\""
    PULSAR_GC: "\"-XX:+UseG1GC -XX:MaxGCPauseMillis=10\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  ## Function service
  ## templates/function-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    headless: yes
    ports:
    - name: http
      port: 6750
    - name: https
      port: 6751
  ## Function PodDisruptionBudget
  ## templates/function-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

stateStorage:
  component: statestorage
  replicaCount: 1
  numStorageContainers: 16
  updateStrategy:
    type: RollingUpdate
  podManagementPolicy: OrderedReady
  probe:
    enabled: yes
    port: 4181
    initial: 10
    period: 30
  #preferredZone: "us-east1-c"
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "false"
    prometheus.io/port: "8080"
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 1Gi
      cpu: 0.5 
  volumes:
    data:
      name: data
      size: 5Gi
      # To override the default_storage class for this volume, set storageClass. 
      # To use an existing storage class, set existingStorageClassName.
      # To have the chart create a storage class, leave existingStorageClassName 
      # unset and specify the storage class parameters. The appropriate parameters
      # will vary by cloud provider. See default_storage above for examples.
      #
      # existingStorageClassName: <name>
      #
      # To use the default storage class of the k8s cluster, use the name "default"
      # existingStorageClassName: default
      #
      # OR (GCP example)
      #
      # storageClass:
      #   provisioner: kubernetes.io/gce-pd
      #   type: pd-ssd
      #   fsType: ext4
      #   extraParams:
      #      replication-type: none
  ## StateStorage configmap
  ## templates/state-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\" -Xms1g -Xmx1g -XX:MaxDirectMemorySize=1g\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  ## Function service
  ## templates/state-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    headless: yes
    ports:
    - name: bkstate
      port: 4181


tokenServer:
  allowedRoles: superuser,token-creation

## Pulsar Extra: Proxy
## templates/proxy-deployment.yaml
##
proxy:
  component: proxy
  replicaCount: 3
  disableZookeeperDiscovery: yes
  updateStrategy: {}
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations:
    prometheus.io/scrape: "true"
    prometheus.io/port: "8080"
  tolerations: []
  #preferredZone: "us-east1-c"
  gracePeriod: 60
  probe:
    enabled: yes
    initial: 10
    period: 30
  # Resources for the main Pulsar proxy
  resources:
    requests:
      memory: 1Gi
      cpu: 1
  # Resources for the websocket proxy
  wsResources:
    requests:
      memory: 1Gi
      cpu: 1
  ## Proxy configmap
  ## templates/proxy-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms1g -Xmx1g -XX:MaxDirectMemorySize=1g\""
    numHttpServerThreads: "10"
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
  wsProxyPort: 8000
  wsProxyPortTls: 8001
  ## Proxy loadbalancer service
  ## templates/proxy-service.yaml
  ##
  # service:
  #   annotations: {}
  #   type: LoadBalancer
  #   ports:
  #   - name: http
  #     port: 8080
  #     protocol: TCP
  #   - name: pulsar
  #     port: 6650
  #     protocol: TCP
  #   - name: ws
  #     port: 8000
  #     protocol: TCP

  # If you are enabling TLS, add these ports:
  #   - name: https
  #     port: 8443
  #     protocol: TCP
  #   - name: pulsarssl
  #     port: 6651
  #     protocol: TCP
  #   - name: wss
  #     port: 8001
  #     protocol: TCP

  # If you are enabling Pulsar Beam, add this port:
  #   - name: pulsarbeam
  #     port: 8085
  #     protocol: TCP


  ## Proxy cluster service
  ## templates/proxy-service.yaml
  ##
  service:
    annotations: {}
    type: ClusterIP
    ports:
    - name: http
      port: 8080
      protocol: TCP
    - name: pulsar
      port: 6650
      protocol: TCP
    - name: ws
      port: 8000
      protocol: TCP



  ## Proxy PodDisruptionBudget
  ## templates/proxy-pdb.yaml
  ##
  pdb:
    usePolicy: yes
    maxUnavailable: 1

## Pulsar Extra:  DNS on proxy RBAC
## templates/dns-rbac.yaml
## templates/dns-deployment.yaml
dns:
  component: dns
  provider: aws
  domainFilter: example.com 
  hostAnnotations: 

## Pulsar Extra: Bookkeeper auto-recovery
## templates/autorecovery-deployment.yaml
##
autoRecovery:
  component: autorecovery
  replicaCount: 1
  #nodeSelector:
  #  cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 512Mi
      cpu: 0.3
  ## Bookkeeper auto-recovery configmap
  ## templates/autorecovery-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\" -Xms512m -Xmx512m \""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"
## Pulsar Extra: Dashboard
## templates/dashboard-deployment.yaml
##
dashboard:
  component: dashboard
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 1024Mi
      cpu: 1
  image:
    repository: apachepulsar/pulsar-dashboard
    pullPolicy: IfNotPresent
  ## Dashboard service
  ## templates/dashboard-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 80

## Components Stack: pulsarexpress
## templates/pulsarexpress.yaml
##
pulsarexpress:
  component: pulsarexpress
  replicaCount: 1
  ingress:
    enabled: yes
    host: ui.host.com
    annotations: {}
  # nodeSelector:
  # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 250Mi
      cpu: 0.1
  configData: {}
  ## Pulsar Express service
  ## templates/pulsarexpress-service.yaml
  ##
  service:
    type: ClusterIP
    annotations: {}
    ports:
      - name: server
        port: 3000

## Pulsar Extra: Zoonavigator
## templates/zoonavigator-deployment.yaml
##
zoonavigator:
  component: zoonavigator
  replicaCount: 1
  autoConnect: true
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 128Mi
      cpu: 0.2
  image:
    repository: 
      web: elkozmon/zoonavigator-web
      api: elkozmon/zoonavigator-api
    pullPolicy: IfNotPresent

  ## Zoonavigator service
  ## templates/zoonavigator-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 8001

  ## Dashboard service
  ## templates/dashboard-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 8000
    - name: jolokia
      port: 8778


## Pulsar Extra: Bastion
## templates/bastion-deployment.yaml
##
bastion:
  component: bastion
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 256Mi
      cpu: 0.25
  ## Bastion configmap
  ## templates/bastion-configmap.yaml
  ##
  configData:
    PULSAR_MEM: "\"-Xms256m -Xmx256m -XX:MaxDirectMemorySize=256m\""
    PULSAR_LOG_LEVEL: "info"
    PULSAR_LOG_ROOT_LEVEL: "info"
    PULSAR_EXTRA_OPTS: "-Dpulsar.log.root.level=info"

## Pulsar-beam webhook broker(s)
## templates/beamwh-deployment.yaml
##
beamwh:
  component: beamwh
  replicaCount: 1
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 256Mi
      cpu: 0.5
  dbType: "inmemory"
  dbRefreshInterval: "60s"
  ## Beamwh configmap
  ## templates/beamwh-configmap.yaml
  ##
  configData: {}
  
## Monitoring Stack: Prometheus
## templates/prometheus-deployment.yaml
##
prometheus:
  component: prometheus
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 512Mi
      cpu: 0.5
  image:
    repository: prom/prometheus
    pullPolicy: IfNotPresent
  volumes:
    data:
      name: data
      size: 5Gi
      # storageClass:
      #   type: gp2
      #   provisioner: kubernetes.io/aws-ebs
      #   fsType: ext4
  ## Prometheus service
  ## templates/prometheus-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 9090

## Monitoring Stack: Grafana
## templates/grafana-deployment.yaml
##
grafana:
  component: grafana
  replicaCount: 1
  # nodeSelector:
    # cloud.google.com/gke-nodepool: default-pool
  annotations: {}
  tolerations: []
  gracePeriod: 60
  resources:
    requests:
      memory: 256Mi
      cpu: 0.25
  image:
    repository: apachepulsar/pulsar-grafana
    pullPolicy: IfNotPresent
  ## Grafana service
  ## templates/grafana-service.yaml
  ##
  service:
    annotations: {}
    ports:
    - name: server
      port: 3000

